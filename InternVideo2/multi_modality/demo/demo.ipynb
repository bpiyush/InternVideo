{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-06 17:05:53,301] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "\n",
    "from config import (Config,\n",
    "                    eval_dict_leaf)\n",
    "\n",
    "from demo_utils import (retrieve_text,\n",
    "                  _frame_from_video,\n",
    "                  setup_internvideo2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = cv2.VideoCapture('example1.mp4')\n",
    "frames = [x for x in _frame_from_video(video)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_candidates = [\"A playful dog and its owner wrestle in the snowy yard, chasing each other with joyous abandon.\",\n",
    "                   \"A man in a gray coat walks through the snowy landscape, pulling a sleigh loaded with toys.\",\n",
    "                   \"A person dressed in a blue jacket shovels the snow-covered pavement outside their house.\",\n",
    "                   \"A pet dog excitedly runs through the snowy yard, chasing a toy thrown by its owner.\",\n",
    "                   \"A person stands on the snowy floor, pushing a sled loaded with blankets, preparing for a fun-filled ride.\",\n",
    "                   \"A man in a gray hat and coat walks through the snowy yard, carefully navigating around the trees.\",\n",
    "                   \"A playful dog slides down a snowy hill, wagging its tail with delight.\",\n",
    "                   \"A person in a blue jacket walks their pet on a leash, enjoying a peaceful winter walk among the trees.\",\n",
    "                   \"A man in a gray sweater plays fetch with his dog in the snowy yard, throwing a toy and watching it run.\",\n",
    "                   \"A person bundled up in a blanket walks through the snowy landscape, enjoying the serene winter scenery.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config.from_file('internvideo2_stage2_config.py')\n",
    "config = eval_dict_leaf(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pth = '/work/piyush/pretrained_checkpoints/LargeModels/InternVideo/1B_clip.pth'\n",
    "config['pretrained_path'] = model_pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"model\"][\"vision_encoder\"][\"pretrained\"] = model_pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['TextEncoders']['bert_large']['config'] = \"../configs/config_bert_large.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/piyush/install/miniconda3/envs/vl/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "odict_keys(['temp', 'vision_encoder.clip_projector.norm1_q.weight', 'vision_encoder.clip_projector.norm1_q.bias', 'vision_encoder.clip_projector.norm1_k.weight', 'vision_encoder.clip_projector.norm1_k.bias', 'vision_encoder.clip_projector.norm1_v.weight', 'vision_encoder.clip_projector.norm1_v.bias', 'vision_encoder.clip_projector.cross_attn.q_bias', 'vision_encoder.clip_projector.cross_attn.k_bias', 'vision_encoder.clip_projector.cross_attn.v_bias', 'vision_encoder.clip_projector.cross_attn.q.weight', 'vision_encoder.clip_projector.cross_attn.k.weight', 'vision_encoder.clip_projector.cross_attn.v.weight', 'vision_encoder.clip_projector.cross_attn.proj.weight', 'vision_encoder.clip_projector.cross_attn.proj.bias', 'text_encoder.transformer.layers.0.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.1.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.2.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.3.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.4.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.5.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.6.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.7.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.8.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.9.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.10.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.11.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.12.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.13.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.14.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.15.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.16.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.17.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.18.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.19.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.20.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.21.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.22.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.23.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.24.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.25.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.26.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.27.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.28.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.29.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.30.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.31.self_attn.rotary_emb.inv_freq'])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m intern_model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43msetup_internvideo2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/InternVideo/InternVideo2/multi_modality/demo/demo_utils.py:106\u001b[0m, in \u001b[0;36msetup_internvideo2\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morigin_num_frames\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(state_dict)\n\u001b[0;32m--> 106\u001b[0m     \u001b[43minterpolate_pos_embed_internvideo2_new\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_without_ddp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_t_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morigin_num_frames\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m a \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(state_dict), state_dict\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m    109\u001b[0m msg \u001b[38;5;241m=\u001b[39m model_without_ddp\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/projects/InternVideo/InternVideo2/multi_modality/models/backbones/internvideo2/pos_embed.py:247\u001b[0m, in \u001b[0;36minterpolate_pos_embed_internvideo2_new\u001b[0;34m(checkpoint_model, model, orig_t_size)\u001b[0m\n\u001b[1;32m    243\u001b[0m         pos_names\u001b[38;5;241m.\u001b[39mappend(k)\n\u001b[1;32m    245\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpos names list for interpolating: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpos_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 247\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pos_names) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, checkpoint_model\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos_embed_spatial\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m checkpoint_model\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos_embed_temporal\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m checkpoint_model\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: odict_keys(['temp', 'vision_encoder.clip_projector.norm1_q.weight', 'vision_encoder.clip_projector.norm1_q.bias', 'vision_encoder.clip_projector.norm1_k.weight', 'vision_encoder.clip_projector.norm1_k.bias', 'vision_encoder.clip_projector.norm1_v.weight', 'vision_encoder.clip_projector.norm1_v.bias', 'vision_encoder.clip_projector.cross_attn.q_bias', 'vision_encoder.clip_projector.cross_attn.k_bias', 'vision_encoder.clip_projector.cross_attn.v_bias', 'vision_encoder.clip_projector.cross_attn.q.weight', 'vision_encoder.clip_projector.cross_attn.k.weight', 'vision_encoder.clip_projector.cross_attn.v.weight', 'vision_encoder.clip_projector.cross_attn.proj.weight', 'vision_encoder.clip_projector.cross_attn.proj.bias', 'text_encoder.transformer.layers.0.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.1.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.2.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.3.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.4.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.5.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.6.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.7.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.8.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.9.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.10.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.11.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.12.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.13.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.14.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.15.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.16.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.17.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.18.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.19.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.20.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.21.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.22.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.23.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.24.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.25.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.26.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.27.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.28.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.29.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.30.self_attn.rotary_emb.inv_freq', 'text_encoder.transformer.layers.31.self_attn.rotary_emb.inv_freq'])"
     ]
    }
   ],
   "source": [
    "intern_model, tokenizer = setup_internvideo2(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../configs/config_bert_large.json\n"
     ]
    }
   ],
   "source": [
    "!ls ../configs/config_bert_large.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, probs = retrieve_text(frames, text_candidates, model=intern_model, topk=5, config=config)\n",
    "\n",
    "for t, p in zip(texts, probs):\n",
    "    print(f'text: {t} ~ prob: {p:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
